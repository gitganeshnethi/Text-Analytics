{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"D:\\\\Datathon\\\\news_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_text      0\n",
      "news_number    0\n",
      "news_type      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data=data.dropna()\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index('news_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['news_type']\n",
    "data = data.drop(['news_type'],axis=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_number</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>727600136.0</th>\n",
       "      <td>Et tu, Rhody?  A recent editorial in the Provi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731714618.0</th>\n",
       "      <td>A recent post in The Farmington Mirror — our t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731714635.0</th>\n",
       "      <td>President Donald Trump, as he often does while...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728627182.0</th>\n",
       "      <td>February is Black History Month, and nothing l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728627443.0</th>\n",
       "      <td>The snow was so heavy, whipped up by gusting w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     news_text\n",
       "news_number                                                   \n",
       "727600136.0  Et tu, Rhody?  A recent editorial in the Provi...\n",
       "731714618.0  A recent post in The Farmington Mirror — our t...\n",
       "731714635.0  President Donald Trump, as he often does while...\n",
       "728627182.0  February is Black History Month, and nothing l...\n",
       "728627443.0  The snow was so heavy, whipped up by gusting w..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('D:\\\\Datathon\\\\dev-INPUT\\\\task1.dev.csv',header=None)\n",
    "data1.columns = ['news_text','news_number','news_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.set_index('news_number')\n",
    "data1 = data1.drop(['news_type'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('D:\\\\Datathon\\\\testdata.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data2=data2.dropna()\n",
    "print(data2.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.columns = ['news_text','news_number','news_type']\n",
    "data2 = data2.set_index('news_number')\n",
    "data2 = data2.drop(['news_type'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_number</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100013.0</th>\n",
       "      <td>Chicago police are seeking the public’s help i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100015.0</th>\n",
       "      <td>Moment of silence at Palestine Pavilion during...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100024.0</th>\n",
       "      <td>The Organization of Islamic Cooperation (OIC) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100031.0</th>\n",
       "      <td>The chairman of the House Oversight and Govern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100040.0</th>\n",
       "      <td>A judge in the United Kingdom has sentenced a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     news_text\n",
       "news_number                                                   \n",
       "100013.0     Chicago police are seeking the public’s help i...\n",
       "100015.0     Moment of silence at Palestine Pavilion during...\n",
       "100024.0     The Organization of Islamic Cooperation (OIC) ...\n",
       "100031.0     The chairman of the House Oversight and Govern...\n",
       "100040.0     A judge in the United Kingdom has sentenced a ..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(data['news_text'],y,test_size=0.3,random_state=42)\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = count_vectorizer.fit(x_train)\n",
    "#print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.fit(x_train)\n",
    "count_train = count_vectorizer.transform(x_train)\n",
    "count_test = count_vectorizer.transform(x_test)\n",
    "count_test1 = count_vectorizer.transform(data1['news_text'])\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(count_train,y_train)\n",
    "predict = clf.predict(count_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float64Index([200017.0, 200036.0, 200038.0, 200086.0, 200113.0], dtype='float64', name='news_number') ['non-propaganda' 'non-propaganda' 'non-propaganda' 'propaganda'\n",
      " 'non-propaganda']\n"
     ]
    }
   ],
   "source": [
    "print(data1.index[:5,],predict[:5,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9544317866073909\n",
      "0.7859007832898172\n"
     ]
    }
   ],
   "source": [
    "accuracy_count_vectorizer = metrics.accuracy_score(y_test,predict)\n",
    "print(accuracy_count_vectorizer)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix( y_test,predict, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_cv_log = (2*pre*rec)/(pre+rec)\n",
    "print(F1_cv_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hash\n",
    "\n",
    "hash_vectorizer = HashingVectorizer(stop_words='english')\n",
    "hash_vectorizer.fit(x_train)\n",
    "hash_train=hash_vectorizer.transform(x_train)\n",
    "hash_test=hash_vectorizer.transform(x_test)\n",
    "clf1 = LogisticRegression()\n",
    "clf1.fit(hash_train,y_train)\n",
    "predict1 = clf1.predict(hash_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.939520237102899\n",
      "0.6743142144638403\n"
     ]
    }
   ],
   "source": [
    "accuracy_hash_vectorizer = metrics.accuracy_score(y_test,predict1)\n",
    "print(accuracy_hash_vectorizer)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, predict1, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_log = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer\n",
    "\n",
    "tfid_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfid_vectorizer.fit(x_train)\n",
    "tfid_train=tfid_vectorizer.transform(x_train)\n",
    "tfid_test=tfid_vectorizer.transform(x_test)\n",
    "#tfid_test1=tfid_vectorizer.transform(data1['news_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = LogisticRegression()\n",
    "clf2.fit(tfid_train,y_train)\n",
    "predict2 = clf2.predict(tfid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9367416875057887\n",
      "0.6451948051948052\n"
     ]
    }
   ],
   "source": [
    "accuracy_tfid_vectorizer = metrics.accuracy_score(y_test,predict2)\n",
    "print(accuracy_tfid_vectorizer)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, predict2, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_tfid_log = (2*pre*rec)/(pre+rec)\n",
    "print(F1_tfid_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passive Aggresive Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9500787255719181\n",
      "0.7707358570820927\n"
     ]
    }
   ],
   "source": [
    "#count_vectorizer\n",
    "\n",
    "pac = PassiveAggressiveClassifier()\n",
    "\n",
    "pac.fit(count_train,y_train)\n",
    "pred = pac.predict(count_test)\n",
    "\n",
    "accuracy_cv = metrics.accuracy_score(y_test,pred)\n",
    "print(accuracy_cv)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, pred, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_cv_pass = (2*pre*rec)/(pre+rec)\n",
    "print(F1_cv_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.957302954524405\n",
      "0.8050739957716702\n"
     ]
    }
   ],
   "source": [
    "#hash_vectorizer\n",
    "\n",
    "pac1 = PassiveAggressiveClassifier()\n",
    "pac1.fit(hash_train,y_train)\n",
    "pred1 = pac1.predict(hash_test)\n",
    "\n",
    "accuracy_hv = metrics.accuracy_score(y_test,pred1)\n",
    "print(accuracy_hv)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, pred1, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_pac = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_pac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9606372140409373\n",
      "0.8170469220835127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhineet Singh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#tfid_vectorizer\n",
    "tfid_test1=tfid_vectorizer.transform(data1['news_text'])\n",
    "pac2 = PassiveAggressiveClassifier()\n",
    "pac2.fit(tfid_train,y_train)\n",
    "pred2 = pac2.predict(tfid_test)\n",
    "\n",
    "accuracy_tfid = metrics.accuracy_score(y_test,pred2)\n",
    "print(accuracy_tfid)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, pred2, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_tfid_pac = (2*pre*rec)/(pre+rec)\n",
    "print(F1_tfid_pac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#dev data\n",
    "\n",
    "tfid_test1=tfid_vectorizer.transform(data1['news_text'])\n",
    "pac2 = PassiveAggressiveClassifier()\n",
    "pac2.fit(tfid_train,y_train)\n",
    "pred2 = pac2.predict(tfid_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predicted1 = pd.DataFrame(data1.index,pred2)\n",
    "dev_predicted1['news_type']=dev_predicted1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predicted = pd.DataFrame({'news_number':dev_predicted1['news_number'],'news_type':dev_predicted1['news_type']}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predicted=dev_predicted.drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predicted.to_csv('dev_predicted.txt',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_test1=tfid_vectorizer.transform(data2['news_text'])\n",
    "pac2 = PassiveAggressiveClassifier()\n",
    "pac2.fit(tfid_train,y_train)\n",
    "pred2 = pac2.predict(tfid_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = pd.DataFrame(data2.index,pred2)\n",
    "test_predicted['news_type']=test_predicted.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted1 = pd.DataFrame({'news_number':test_predicted['news_number'],'news_type':test_predicted['news_type']}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted1 = test_predicted1.drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted1.to_csv('test_predicted.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9285912753542651\n",
      "0.7178924259055983\n"
     ]
    }
   ],
   "source": [
    "#count_vectorizer\n",
    "\n",
    "nav = MultinomialNB()\n",
    "nav.fit(count_train,y_train)\n",
    "prediction = nav.predict(count_test)\n",
    "\n",
    "accuracy_c_v = metrics.accuracy_score(y_test,prediction)\n",
    "print(accuracy_c_v)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_cv_nav = (2*pre*rec)/(pre+rec)\n",
    "print(F1_cv_nav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-65a326fe0e5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnav1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnav1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhash_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprediction1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnav1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhash_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    602\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[0;32m    603\u001b[0m                                        dtype=np.float64)\n\u001b[1;32m--> 604\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    706\u001b[0m         \u001b[1;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input X must be non-negative\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    709\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "#hash_vectorizer\n",
    "\n",
    "nav1 = MultinomialNB()\n",
    "nav1.fit(hash_train,y_train)\n",
    "prediction1 = nav1.predict(hash_test)\n",
    "\n",
    "accuracy_h_v = metrics.accuracy_score(y_test,prediction1)\n",
    "print(accuracy_h_v)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction1, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_nav = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_nav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8837640085208854\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfid_vectorizer\n",
    "nav2 = MultinomialNB()\n",
    "nav2.fit(tfid_train,y_train)\n",
    "prediction2 = nav2.predict(tfid_test)\n",
    "\n",
    "accuracy_tfid1 = metrics.accuracy_score(y_test,prediction2)\n",
    "print(accuracy_tfid1)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction2, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_tfid_nav2 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_tfid_nav2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9054366953783458\n",
      "0.33571893298633704\n"
     ]
    }
   ],
   "source": [
    "#count_vectorizer\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(count_train,y_train)\n",
    "prediction_rf = rf.predict(count_test)\n",
    "\n",
    "accuracy_c_v = metrics.accuracy_score(y_test,prediction_rf)\n",
    "print(accuracy_c_v)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_rf, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_cv_rf = (2*pre*rec)/(pre+rec)\n",
    "print(F1_cv_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8986755580253775\n",
      "0.2306610407876231\n"
     ]
    }
   ],
   "source": [
    "#hash_vectorizer\n",
    "\n",
    "rf1 = RandomForestClassifier()\n",
    "rf1.fit(hash_train,y_train)\n",
    "prediction_rf1 = rf1.predict(hash_test)\n",
    "\n",
    "accuracy_h_v = metrics.accuracy_score(y_test,prediction_rf1)\n",
    "print(accuracy_h_v)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_rf1, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_rf1 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_rf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9049736037788274\n",
      "0.3214285714285714\n"
     ]
    }
   ],
   "source": [
    "#tfid_vectorizer\n",
    "rf2 = RandomForestClassifier()\n",
    "rf2.fit(tfid_train,y_train)\n",
    "prediction_rf2 = rf2.predict(tfid_test)\n",
    "\n",
    "accuracy_tfid1 = metrics.accuracy_score(y_test,prediction_rf2)\n",
    "print(accuracy_tfid1)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_rf2, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_tfid_rf2 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_tfid_rf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9312772066314717\n",
      "0.6818181818181819\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#count_vectorizer\n",
    "\n",
    "dtf = DecisionTreeClassifier()\n",
    "dtf.fit(count_train,y_train)\n",
    "prediction_dtf = dtf.predict(count_test)\n",
    "\n",
    "accuracy_c_v = metrics.accuracy_score(y_test,prediction_dtf)\n",
    "print(accuracy_c_v)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_dtf, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_cv_dtf = (2*pre*rec)/(pre+rec)\n",
    "print(F1_cv_dtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.924886542558118\n",
      "0.6652909616178291\n"
     ]
    }
   ],
   "source": [
    "#hash_vectorizer\n",
    "\n",
    "dtf1 = DecisionTreeClassifier()\n",
    "dtf1.fit(hash_train,y_train)\n",
    "prediction_dtf1 = dtf1.predict(hash_test)\n",
    "\n",
    "accuracy_h_v = metrics.accuracy_score(y_test,prediction_dtf1)\n",
    "print(accuracy_h_v)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_dtf1, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_dtf1 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_dtf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9171992220061128\n",
      "0.6320987654320986\n"
     ]
    }
   ],
   "source": [
    "#tfid_vectorizer\n",
    "dtf2 = DecisionTreeClassifier()\n",
    "dtf2.fit(tfid_train,y_train)\n",
    "prediction_dtf2 = dtf2.predict(tfid_test)\n",
    "\n",
    "accuracy_tfid1 = metrics.accuracy_score(y_test,prediction_dtf2)\n",
    "print(accuracy_tfid1)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_dtf2, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_tfid_dtf2 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_tfid_dtf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-NN Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8948782069093266\n",
      "0.19560595322466337\n"
     ]
    }
   ],
   "source": [
    "#count_vectorizer\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(count_train,y_train)\n",
    "prediction_knn = knn.predict(count_test)\n",
    "\n",
    "accuracy_c_v = metrics.accuracy_score(y_test,prediction_knn)\n",
    "print(accuracy_c_v)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_knn, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_cv_knn = (2*pre*rec)/(pre+rec)\n",
    "print(F1_cv_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9227563212003335\n",
      "0.580060422960725\n"
     ]
    }
   ],
   "source": [
    "#hash_vectorizer\n",
    "\n",
    "knn1 = KNeighborsClassifier()\n",
    "knn1.fit(hash_train,y_train)\n",
    "prediction_knn1 = knn1.predict(hash_test)\n",
    "\n",
    "accuracy_h_v = metrics.accuracy_score(y_test,prediction_knn1)\n",
    "print(accuracy_h_v)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_knn1, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_knn1 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_knn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9204408632027415\n",
      "0.5551527705851891\n"
     ]
    }
   ],
   "source": [
    "#tfid_vectorizer\n",
    "knn2 = KNeighborsClassifier()\n",
    "knn2.fit(tfid_train,y_train)\n",
    "prediction_knn2 = knn2.predict(tfid_test)\n",
    "\n",
    "accuracy_tfid1 = metrics.accuracy_score(y_test,prediction_knn2)\n",
    "print(accuracy_tfid1)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_knn2, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_tfid_knn2 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_tfid_knn2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8859868481985737\n",
      "0.037529319781078964\n"
     ]
    }
   ],
   "source": [
    "#count_vectorizer\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(count_train,y_train)\n",
    "prediction_svm = svm.predict(count_test)\n",
    "\n",
    "accuracy_c_v = metrics.accuracy_score(y_test,prediction_svm)\n",
    "print(accuracy_c_v)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_svm, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_cv_svm = (2*pre*rec)/(pre+rec)\n",
    "print(F1_cv_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8837640085208854\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "#hash_vectorizer\n",
    "\n",
    "svm1 = SVC()\n",
    "svm1.fit(hash_train,y_train)\n",
    "prediction_svm1 = svm1.predict(hash_test)\n",
    "\n",
    "accuracy_h_v = metrics.accuracy_score(y_test,prediction_svm1)\n",
    "print(accuracy_h_v)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_svm1, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_svm1 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_svm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8837640085208854\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#tfid_vectorizer\n",
    "svm2 = SVC()\n",
    "svm2.fit(tfid_train,y_train)\n",
    "prediction_svm2 = svm2.predict(tfid_test)\n",
    "\n",
    "accuracy_tfid1 = metrics.accuracy_score(y_test,prediction_svm2)\n",
    "print(accuracy_tfid1)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_svm2, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_tfid_svm2 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_tfid_svm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9410947485412615\n",
      "0.6991485335856196\n"
     ]
    }
   ],
   "source": [
    "#count_vectorizer\n",
    "\n",
    "xg = AdaBoostClassifier()\n",
    "xg.fit(count_train,y_train)\n",
    "prediction_xg = xg.predict(count_test)\n",
    "\n",
    "accuracy_c_v = metrics.accuracy_score(y_test,prediction_xg)\n",
    "print(accuracy_c_v)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_xg, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_cv_xg = (2*pre*rec)/(pre+rec)\n",
    "print(F1_cv_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9360933592664629\n",
      "0.6793680297397768\n"
     ]
    }
   ],
   "source": [
    "#hash_vectorizer\n",
    "\n",
    "xg1 = AdaBoostClassifier()\n",
    "xg1.fit(hash_train,y_train)\n",
    "prediction_xg1 = xg1.predict(hash_test)\n",
    "\n",
    "accuracy_h_v = metrics.accuracy_score(y_test,prediction_xg1)\n",
    "print(accuracy_h_v)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_xg1, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_xg1 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_xg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9379457256645365\n",
      "0.6918123275068997\n"
     ]
    }
   ],
   "source": [
    "#tfid_vectorizer\n",
    "xg2 = AdaBoostClassifier()\n",
    "xg2.fit(tfid_train,y_train)\n",
    "prediction_xg2 = xg2.predict(tfid_test)\n",
    "\n",
    "accuracy_tfid1 = metrics.accuracy_score(y_test,prediction_xg2)\n",
    "print(accuracy_tfid1)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_xg2, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_tfid_xg2 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_tfid_xg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Layer Perceptron Classification Model (Feed Forward Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-Layer Perceptron Classifier - Neural Networks\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(5))\n",
    "mlp.fit(hash_train, y_train)\n",
    "pred = mlp.predict(hash_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, pred, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_mlp = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9613781606001667\n",
      "0.8170250109697235\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(5,5))\n",
    "mlp.fit(tfid_train, y_train)\n",
    "pred = mlp.predict(tfid_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, pred, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_mlp = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.961007687320552\n",
      "0.8147822261328641\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(5,5,5))\n",
    "mlp.fit(tfid_train, y_train)\n",
    "pred = mlp.predict(tfid_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, pred, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_mlp = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9601741224414189\n",
      "0.8099027409372237\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(5,5,5))\n",
    "mlp.fit(count_train, y_train)\n",
    "pred = mlp.predict(count_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, pred, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_mlp = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
