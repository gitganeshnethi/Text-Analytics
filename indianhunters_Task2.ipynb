{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "os.chdir('C:\\\\Users\\\\Abhineet Singh\\\\datathon\\\\test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('article111111111.txt')\n",
    "articles = f.readlines()\n",
    "#t = pd.read_csv('article111111112.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('D://DATATHON//output//test-INPUT//test-INPUT//tasks-2-3//test')\n",
    "\n",
    "#pd.concat([a1,a2],axis =0,ignore_index=True)\n",
    "\n",
    "filenames = os.listdir()\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = list(filter(lambda x: x[-4:] == '.txt', filenames))\n",
    "len(txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.DataFrame()\n",
    "for name in txt_files:\n",
    "    a = open(name, encoding= 'utf-8').readlines()\n",
    "    df = pd.DataFrame(a,columns=['text'])\n",
    "    articles = pd.concat([articles,df],axis =0 ,ignore_index = True)\n",
    "\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_files = list(filter(lambda x: x[-12:] == 'task2.labels', filenames))\n",
    "len(lab_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_df = pd.DataFrame()\n",
    "\n",
    "for name in lab_files:\n",
    "    labs = open(name).readlines()\n",
    "    l1=[]\n",
    "    for l in labs:\n",
    "        l1.append(l.split('\\t'))\n",
    "    df = pd.DataFrame(l1,columns = ['a','b','c'])    \n",
    "    lable_df = pd.concat([lable_df,df],axis =0 ,ignore_index = True)\n",
    "\n",
    "lable_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lable_df['c'] = lable_df['c'].str.replace('\\n','')\n",
    "train=pd.concat([articles['text'],lable_df['a'],lable_df['c']],axis=1,ignore_index=True)\n",
    "\n",
    "train.columns = ['news_text','news_number','news_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['news_type'] = train['news_type'].str.replace('\\n','')\n",
    "train['news_text'] = train['news_text'].str.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index(['news_number',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('C:\\\\Users\\\\Abhineet Singh\\\\datathonTask_2_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Task_2_train = pd.read_csv(\"D:/DATATHON/output/test-INPUT/test-INPUT/tasks-2-3/Task_2_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Task_2_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0     0\n",
      "news_text      0\n",
      "news_number    0\n",
      "news_type      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data=data.dropna()\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index('news_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_text</th>\n",
       "      <th>news_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111111112</th>\n",
       "      <td>US bloggers banned from entering UK</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111111112</th>\n",
       "      <td>Two prominent US bloggers have been banned fro...</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111111112</th>\n",
       "      <td>Pamela Geller and Robert Spencer co-founded an...</td>\n",
       "      <td>propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111111112</th>\n",
       "      <td>They were due to speak at an English Defence L...</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111111112</th>\n",
       "      <td>A government spokesman said individuals whose ...</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     news_text       news_type\n",
       "news_number                                                                   \n",
       "111111112                  US bloggers banned from entering UK  non-propaganda\n",
       "111111112    Two prominent US bloggers have been banned fro...  non-propaganda\n",
       "111111112    Pamela Geller and Robert Spencer co-founded an...      propaganda\n",
       "111111112    They were due to speak at an English Defence L...  non-propaganda\n",
       "111111112    A government spokesman said individuals whose ...  non-propaganda"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['news_type']\n",
    "data = data.drop('news_type',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(data['news_text'],y,test_size=0.3,random_state=42)\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = count_vectorizer.fit(x_train)\n",
    "#print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhineet Singh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer.fit(x_train)\n",
    "count_train = count_vectorizer.transform(x_train)\n",
    "count_test = count_vectorizer.transform(x_test)\n",
    "count_test1 = count_vectorizer.transform(data['news_text'])\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(count_train,y_train)\n",
    "predict = clf.predict(count_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([111111112, 111111112, 111111112, 111111112, 111111112], dtype='int64', name='news_number') ['propaganda' 'non-propaganda' 'non-propaganda' 'non-propaganda'\n",
      " 'non-propaganda']\n"
     ]
    }
   ],
   "source": [
    "print(data.index[:5,],predict[:5,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7543818649217107\n",
      "0.4511749347258486\n"
     ]
    }
   ],
   "source": [
    "accuracy_count_vectorizer = metrics.accuracy_score(y_test,predict)\n",
    "print(accuracy_count_vectorizer)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix( y_test,predict, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_cv_log = (2*pre*rec)/(pre+rec)\n",
    "print(F1_cv_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhineet Singh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#hash\n",
    "\n",
    "hash_vectorizer = HashingVectorizer(stop_words='english')\n",
    "hash_vectorizer.fit(x_train)\n",
    "hash_train=hash_vectorizer.transform(x_train)\n",
    "hash_test=hash_vectorizer.transform(x_test)\n",
    "clf1 = LogisticRegression()\n",
    "clf1.fit(hash_train,y_train)\n",
    "predict1 = clf1.predict(hash_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7410609955597102\n",
      "0.2503382949932341\n"
     ]
    }
   ],
   "source": [
    "accuracy_hash_vectorizer = metrics.accuracy_score(y_test,predict1)\n",
    "print(accuracy_hash_vectorizer)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, predict1, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_log = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer\n",
    "\n",
    "tfid_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfid_vectorizer.fit(x_train)\n",
    "tfid_train=tfid_vectorizer.transform(x_train)\n",
    "tfid_test=tfid_vectorizer.transform(x_test)\n",
    "#tfid_test1=tfid_vectorizer.transform(data1['news_text'])\n",
    "clf2 = LogisticRegression()\n",
    "clf2.fit(tfid_train,y_train)\n",
    "predict2 = clf2.predict(tfid_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7412946950222015\n",
      "0.24229979466119098\n"
     ]
    }
   ],
   "source": [
    "accuracy_tfid_vectorizer = metrics.accuracy_score(y_test,predict2)\n",
    "print(accuracy_tfid_vectorizer)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, predict2, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_tfid_log = (2*pre*rec)/(pre+rec)\n",
    "print(F1_tfid_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passive Aggresive Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7123159616732881\n",
      "0.44222927050294514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhineet Singh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#count_vectorizer\n",
    "\n",
    "pac = PassiveAggressiveClassifier()\n",
    "\n",
    "pac.fit(count_train,y_train)\n",
    "pred = pac.predict(count_test)\n",
    "\n",
    "accuracy_cv = metrics.accuracy_score(y_test,pred)\n",
    "print(accuracy_cv)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, pred, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_cv_pass = (2*pre*rec)/(pre+rec)\n",
    "print(F1_cv_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7300771208226221\n",
      "0.4660194174757281\n"
     ]
    }
   ],
   "source": [
    "#hash_vectorizer\n",
    "\n",
    "pac1 = PassiveAggressiveClassifier()\n",
    "pac1.fit(hash_train,y_train)\n",
    "pred1 = pac1.predict(hash_test)\n",
    "\n",
    "accuracy_hv = metrics.accuracy_score(y_test,pred1)\n",
    "print(accuracy_hv)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, pred1, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_pac = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_pac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7176910493105866\n",
      "0.4621549421193233\n"
     ]
    }
   ],
   "source": [
    "#tfid_vectorizer\n",
    "tfid_test1=tfid_vectorizer.transform(data['news_text'])\n",
    "pac2 = PassiveAggressiveClassifier()\n",
    "pac2.fit(tfid_train,y_train)\n",
    "pred2 = pac2.predict(tfid_test)\n",
    "\n",
    "accuracy_tfid = metrics.accuracy_score(y_test,pred2)\n",
    "print(accuracy_tfid)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, pred2, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_tfid_pac = (2*pre*rec)/(pre+rec)\n",
    "print(F1_tfid_pac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev data\n",
    "\n",
    "tfid_test1=tfid_vectorizer.transform(data1['news_text'])\n",
    "pac2 = PassiveAggressiveClassifier()\n",
    "pac2.fit(tfid_train,y_train)\n",
    "pred2 = pac2.predict(tfid_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predicted1 = pd.DataFrame(data1.index,pred2)\n",
    "dev_predicted1['news_type']=dev_predicted1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predicted = pd.DataFrame({'news_number':dev_predicted1['news_number'],'news_type':dev_predicted1['news_type']},columns=['news_number','news_type']).reset_index()\n",
    "#dev_predicted.columns.keys = ['news_number','news_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_number</th>\n",
       "      <th>news_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200017.0</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200036.0</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200038.0</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200086.0</td>\n",
       "      <td>propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200113.0</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   news_number       news_type\n",
       "0     200017.0  non-propaganda\n",
       "1     200036.0  non-propaganda\n",
       "2     200038.0  non-propaganda\n",
       "3     200086.0      propaganda\n",
       "4     200113.0  non-propaganda"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dev_predicted=dev_predicted.drop(['index'],axis=1)\n",
    "dev_predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predicted.to_csv('dev_predicted2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7515774713718159\n",
      "0.4959696538643907\n"
     ]
    }
   ],
   "source": [
    "#count_vectorizer\n",
    "\n",
    "nav = MultinomialNB()\n",
    "nav.fit(count_train,y_train)\n",
    "prediction = nav.predict(count_test)\n",
    "\n",
    "accuracy_c_v = metrics.accuracy_score(y_test,prediction)\n",
    "print(accuracy_c_v)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_cv_nav = (2*pre*rec)/(pre+rec)\n",
    "print(F1_cv_nav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-65a326fe0e5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnav1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnav1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhash_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprediction1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnav1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhash_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    608\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[0;32m    609\u001b[0m                                        dtype=np.float64)\n\u001b[1;32m--> 610\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[1;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input X must be non-negative\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "#hash_vectorizer\n",
    "\n",
    "nav1 = MultinomialNB()\n",
    "nav1.fit(hash_train,y_train)\n",
    "prediction1 = nav1.predict(hash_test)\n",
    "\n",
    "accuracy_h_v = metrics.accuracy_score(y_test,prediction1)\n",
    "print(accuracy_h_v)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction1, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_nav = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_nav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7338163122224819\n",
      "0.11773818745158793\n"
     ]
    }
   ],
   "source": [
    "#tfid_vectorizer\n",
    "nav2 = MultinomialNB()\n",
    "nav2.fit(tfid_train,y_train)\n",
    "prediction2 = nav2.predict(tfid_test)\n",
    "\n",
    "accuracy_tfid1 = metrics.accuracy_score(y_test,prediction2)\n",
    "print(accuracy_tfid1)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction2, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_tfid_nav2 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_tfid_nav2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhineet Singh\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7258705304977798\n",
      "0.40244523688232303\n"
     ]
    }
   ],
   "source": [
    "#count_vectorizer\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(count_train,y_train)\n",
    "prediction_rf = rf.predict(count_test)\n",
    "\n",
    "accuracy_c_v = metrics.accuracy_score(y_test,prediction_rf)\n",
    "print(accuracy_c_v)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_rf, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_cv_rf = (2*pre*rec)/(pre+rec)\n",
    "print(F1_cv_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhineet Singh\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7356859079224117\n",
      "0.26510721247563357\n"
     ]
    }
   ],
   "source": [
    "#hash_vectorizer\n",
    "\n",
    "rf1 = RandomForestClassifier()\n",
    "rf1.fit(hash_train,y_train)\n",
    "prediction_rf1 = rf1.predict(hash_test)\n",
    "\n",
    "accuracy_h_v = metrics.accuracy_score(y_test,prediction_rf1)\n",
    "print(accuracy_h_v)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_rf1, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_rf1 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_rf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhineet Singh\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7356859079224117\n",
      "0.3132969034608379\n"
     ]
    }
   ],
   "source": [
    "#tfid_vectorizer\n",
    "rf2 = RandomForestClassifier()\n",
    "rf2.fit(tfid_train,y_train)\n",
    "prediction_rf2 = rf2.predict(tfid_test)\n",
    "\n",
    "accuracy_tfid1 = metrics.accuracy_score(y_test,prediction_rf2)\n",
    "print(accuracy_tfid1)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_rf2, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_tfid_rf2 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_tfid_rf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.699228791773779\n",
      "0.4626304801670146\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#count_vectorizer\n",
    "\n",
    "dtf = DecisionTreeClassifier()\n",
    "dtf.fit(count_train,y_train)\n",
    "prediction_dtf = dtf.predict(count_test)\n",
    "\n",
    "accuracy_c_v = metrics.accuracy_score(y_test,prediction_dtf)\n",
    "print(accuracy_c_v)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_dtf, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_cv_dtf = (2*pre*rec)/(pre+rec)\n",
    "print(F1_cv_dtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7001635896237439\n",
      "0.42492155983863733\n"
     ]
    }
   ],
   "source": [
    "#hash_vectorizer\n",
    "\n",
    "dtf1 = DecisionTreeClassifier()\n",
    "dtf1.fit(hash_train,y_train)\n",
    "prediction_dtf1 = dtf1.predict(hash_test)\n",
    "\n",
    "accuracy_h_v = metrics.accuracy_score(y_test,prediction_dtf1)\n",
    "print(accuracy_h_v)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_dtf1, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_dtf1 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_dtf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6978265949988315\n",
      "0.4225100491290755\n"
     ]
    }
   ],
   "source": [
    "#tfid_vectorizer\n",
    "dtf2 = DecisionTreeClassifier()\n",
    "dtf2.fit(tfid_train,y_train)\n",
    "prediction_dtf2 = dtf2.predict(tfid_test)\n",
    "\n",
    "accuracy_tfid1 = metrics.accuracy_score(y_test,prediction_dtf2)\n",
    "print(accuracy_tfid1)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_dtf2, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_tfid_dtf2 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_tfid_dtf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-NN Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7188595466230427\n",
      "0.12889210716871832\n"
     ]
    }
   ],
   "source": [
    "#count_vectorizer\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(count_train,y_train)\n",
    "prediction_knn = knn.predict(count_test)\n",
    "\n",
    "accuracy_c_v = metrics.accuracy_score(y_test,prediction_knn)\n",
    "print(accuracy_c_v)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_knn, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_cv_knn = (2*pre*rec)/(pre+rec)\n",
    "print(F1_cv_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7176910493105866\n",
      "0.050314465408805034\n"
     ]
    }
   ],
   "source": [
    "#hash_vectorizer\n",
    "\n",
    "knn1 = KNeighborsClassifier()\n",
    "knn1.fit(hash_train,y_train)\n",
    "prediction_knn1 = knn1.predict(hash_test)\n",
    "\n",
    "accuracy_h_v = metrics.accuracy_score(y_test,prediction_knn1)\n",
    "print(accuracy_h_v)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_knn1, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_knn1 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_knn1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Test data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = pd.read_csv('C:\\\\Users\\\\Abhineet Singh\\\\datathon\\\\datathonTask_2_test.csv')\n",
    "test2 = test2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = test2.drop(['news_type'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3  = test3.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = test3.set_index('news_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_number</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111111111</th>\n",
       "      <td>Next plague outbreak in Madagascar could be 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111111111</th>\n",
       "      <td>Geneva - The World Health Organisation chief o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111111111</th>\n",
       "      <td>\"The next transmission could be more pronounce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111111111</th>\n",
       "      <td>An outbreak of both bubonic plague, which is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111111111</th>\n",
       "      <td>Madagascar has suffered bubonic plague outbrea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     news_text\n",
       "news_number                                                   \n",
       "111111111    Next plague outbreak in Madagascar could be 's...\n",
       "111111111    Geneva - The World Health Organisation chief o...\n",
       "111111111    \"The next transmission could be more pronounce...\n",
       "111111111    An outbreak of both bubonic plague, which is s...\n",
       "111111111    Madagascar has suffered bubonic plague outbrea..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hash\n",
    "\n",
    "hash_test1=hash_vectorizer.transform(test3['news_text'])\n",
    "clf1 = KNeighborsClassifier()\n",
    "clf1.fit(hash_train,y_train)\n",
    "predict1 = clf1.predict(hash_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "task2_test_predicted = pd.DataFrame({'news_number':test3.index,'Line_id':test2['Unnamed: 0'],'news_type':predict1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_number</th>\n",
       "      <th>Line_id</th>\n",
       "      <th>news_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111111111</td>\n",
       "      <td>0</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111111111</td>\n",
       "      <td>2</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111111111</td>\n",
       "      <td>4</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>111111111</td>\n",
       "      <td>6</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>111111111</td>\n",
       "      <td>8</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   news_number  Line_id       news_type\n",
       "0    111111111        0  non-propaganda\n",
       "2    111111111        2  non-propaganda\n",
       "4    111111111        4  non-propaganda\n",
       "6    111111111        6  non-propaganda\n",
       "8    111111111        8  non-propaganda"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task2_test_predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "task2_test_predicted.to_csv('Task2_test_predicted.csv',index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7204954428604814\n",
      "0.052297939778129965\n"
     ]
    }
   ],
   "source": [
    "#tfid_vectorizer\n",
    "knn2 = KNeighborsClassifier()\n",
    "knn2.fit(tfid_train,y_train)\n",
    "prediction_knn2 = knn2.predict(tfid_test)\n",
    "\n",
    "accuracy_tfid1 = metrics.accuracy_score(y_test,prediction_knn2)\n",
    "print(accuracy_tfid1)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_knn2, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_tfid_knn2 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_tfid_knn2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhineet Singh\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7197943444730077\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhineet Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#count_vectorizer\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(count_train,y_train)\n",
    "prediction_svm = svm.predict(count_test)\n",
    "\n",
    "accuracy_c_v = metrics.accuracy_score(y_test,prediction_svm)\n",
    "print(accuracy_c_v)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_svm, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_cv_svm = (2*pre*rec)/(pre+rec)\n",
    "print(F1_cv_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7197943444730077\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhineet Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "#hash_vectorizer\n",
    "\n",
    "svm1 = SVC()\n",
    "svm1.fit(hash_train,y_train)\n",
    "prediction_svm1 = svm1.predict(hash_test)\n",
    "\n",
    "accuracy_h_v = metrics.accuracy_score(y_test,prediction_svm1)\n",
    "print(accuracy_h_v)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_svm1, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_svm1 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_svm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7197943444730077\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhineet Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#tfid_vectorizer\n",
    "svm2 = SVC()\n",
    "svm2.fit(tfid_train,y_train)\n",
    "prediction_svm2 = svm2.predict(tfid_test)\n",
    "\n",
    "accuracy_tfid1 = metrics.accuracy_score(y_test,prediction_svm2)\n",
    "print(accuracy_tfid1)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_svm2, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_tfid_svm2 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_tfid_svm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7314793175975696\n",
      "0.25048923679060664\n"
     ]
    }
   ],
   "source": [
    "#count_vectorizer\n",
    "\n",
    "xg = AdaBoostClassifier()\n",
    "xg.fit(count_train,y_train)\n",
    "prediction_xg = xg.predict(count_test)\n",
    "\n",
    "accuracy_c_v = metrics.accuracy_score(y_test,prediction_xg)\n",
    "print(accuracy_c_v)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_xg, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_cv_xg = (2*pre*rec)/(pre+rec)\n",
    "print(F1_cv_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7293760224351484\n",
      "0.23915900131406045\n"
     ]
    }
   ],
   "source": [
    "#hash_vectorizer\n",
    "\n",
    "xg1 = AdaBoostClassifier()\n",
    "xg1.fit(hash_train,y_train)\n",
    "prediction_xg1 = xg1.predict(hash_test)\n",
    "\n",
    "accuracy_h_v = metrics.accuracy_score(y_test,prediction_xg1)\n",
    "print(accuracy_h_v)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_xg1, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_xg1 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_xg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7300771208226221\n",
      "0.2553191489361702\n"
     ]
    }
   ],
   "source": [
    "#tfid_vectorizer\n",
    "xg2 = AdaBoostClassifier()\n",
    "xg2.fit(tfid_train,y_train)\n",
    "prediction_xg2 = xg2.predict(tfid_test)\n",
    "\n",
    "accuracy_tfid1 = metrics.accuracy_score(y_test,prediction_xg2)\n",
    "print(accuracy_tfid1)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, prediction_xg2, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_tfid_xg2 = (2*pre*rec)/(pre+rec)\n",
    "print(F1_tfid_xg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Layer Perceptron Classification Model (Feed Forward Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7015657863986913\n",
      "0.4421144604630843\n"
     ]
    }
   ],
   "source": [
    "#Multi-Layer Perceptron Classifier - Neural Networks\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(5,5,5))\n",
    "mlp.fit(hash_train, y_train)\n",
    "pred = mlp.predict(hash_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, pred, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_mlp = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9613781606001667\n",
      "0.8170250109697235\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(5,5))\n",
    "mlp.fit(tfid_train, y_train)\n",
    "pred = mlp.predict(tfid_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, pred, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_mlp = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.961007687320552\n",
      "0.8147822261328641\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(5,5,5))\n",
    "mlp.fit(tfid_train, y_train)\n",
    "pred = mlp.predict(tfid_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, pred, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_mlp = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9601741224414189\n",
      "0.8099027409372237\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(5,5,5))\n",
    "mlp.fit(count_train, y_train)\n",
    "pred = mlp.predict(count_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = metrics.confusion_matrix(y_test, pred, labels=['non-propaganda', 'propaganda']).ravel()\n",
    "rec = (tp/(tp+fn)) # recall/sensitivity\n",
    "pre = (tp/(tp+fp)) # precision\n",
    "F1_hv_mlp = (2*pre*rec)/(pre+rec)\n",
    "print(F1_hv_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
